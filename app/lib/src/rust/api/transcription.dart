// This file is automatically generated, so please do not edit it.
// @generated by `flutter_rust_bridge`@ 2.11.1.

// ignore_for_file: invalid_use_of_internal_member, unused_import, unnecessary_import

import '../frb_generated.dart';
import 'package:flutter_rust_bridge/flutter_rust_bridge_for_generated.dart';

// These functions are ignored because they are not marked as `pub`: `engine`, `format_timestamp`, `process_audio_chunk`
// These types are ignored because they are neither used by any `pub` functions nor (for structs and enums) marked `#[frb(unignore)]`: `TranscriptionEngine`, `TranscriptionStatus`
// These function are ignored because they are on traits that is not defined in current crate (put an empty `#[frb]` on it to unignore): `clone`, `clone`, `clone`, `eq`, `fmt`, `fmt`, `fmt`

/// Initialize the transcription engine with the given config.
///
/// Downloads/loads the Whisper model. This may take time on first run.
Future<void> initTranscription({
  required String modelSize,
  required String language,
  required bool translateToEnglish,
  required bool useGpu,
}) => RustLib.instance.api.crateApiTranscriptionInitTranscription(
  modelSize: modelSize,
  language: language,
  translateToEnglish: translateToEnglish,
  useGpu: useGpu,
);

/// Start a transcription session for a call.
Future<void> startTranscription({required String callId}) => RustLib
    .instance
    .api
    .crateApiTranscriptionStartTranscription(callId: callId);

/// Feed raw PCM audio data (f32, 16kHz, mono) to the transcription engine.
///
/// The `speaker_track_id` identifies which WebRTC track this audio came from,
/// enabling per-speaker attribution without ML diarization.
///
/// Returns any new transcript segments produced.
Future<List<TranscriptSegment>> feedAudio({
  required List<double> audioData,
  required String speakerTrackId,
}) => RustLib.instance.api.crateApiTranscriptionFeedAudio(
  audioData: audioData,
  speakerTrackId: speakerTrackId,
);

/// Map a WebRTC audio track ID to a Nostr identity.
Future<void> registerSpeaker({
  required String trackId,
  required String pubkeyHex,
  required String displayName,
}) => RustLib.instance.api.crateApiTranscriptionRegisterSpeaker(
  trackId: trackId,
  pubkeyHex: pubkeyHex,
  displayName: displayName,
);

/// Pause transcription (e.g., when user requests privacy).
Future<void> pauseTranscription() =>
    RustLib.instance.api.crateApiTranscriptionPauseTranscription();

/// Resume transcription after pause.
Future<void> resumeTranscription() =>
    RustLib.instance.api.crateApiTranscriptionResumeTranscription();

/// Stop transcription and return the full transcript.
Future<List<TranscriptSegment>> stopTranscription() =>
    RustLib.instance.api.crateApiTranscriptionStopTranscription();

/// Get the current transcription status.
Future<String> getTranscriptionStatus() =>
    RustLib.instance.api.crateApiTranscriptionGetTranscriptionStatus();

/// Get all transcript segments for the current session.
Future<List<TranscriptSegment>> getTranscriptSegments() =>
    RustLib.instance.api.crateApiTranscriptionGetTranscriptSegments();

/// Get transcript as formatted text (for export/display).
Future<String> getTranscriptText() =>
    RustLib.instance.api.crateApiTranscriptionGetTranscriptText();

/// Search transcript segments by text query.
Future<List<TranscriptSegment>> searchTranscript({required String query}) =>
    RustLib.instance.api.crateApiTranscriptionSearchTranscript(query: query);

/// Transcription segment with timing and speaker info.
class TranscriptSegment {
  /// Unique segment ID.
  final String id;

  /// Speaker identifier (Nostr pubkey hex or "unknown").
  final String speakerId;

  /// Human-readable speaker name.
  final String speakerName;

  /// Transcribed text content.
  final String text;

  /// Start time in milliseconds from call start.
  final PlatformInt64 startMs;

  /// End time in milliseconds from call start.
  final PlatformInt64 endMs;

  /// Confidence score 0.0-1.0.
  final double confidence;

  /// Language code (e.g., "en", "es").
  final String language;

  /// Whether this is a final (non-interim) result.
  final bool isFinal;

  const TranscriptSegment({
    required this.id,
    required this.speakerId,
    required this.speakerName,
    required this.text,
    required this.startMs,
    required this.endMs,
    required this.confidence,
    required this.language,
    required this.isFinal,
  });

  @override
  int get hashCode =>
      id.hashCode ^
      speakerId.hashCode ^
      speakerName.hashCode ^
      text.hashCode ^
      startMs.hashCode ^
      endMs.hashCode ^
      confidence.hashCode ^
      language.hashCode ^
      isFinal.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is TranscriptSegment &&
          runtimeType == other.runtimeType &&
          id == other.id &&
          speakerId == other.speakerId &&
          speakerName == other.speakerName &&
          text == other.text &&
          startMs == other.startMs &&
          endMs == other.endMs &&
          confidence == other.confidence &&
          language == other.language &&
          isFinal == other.isFinal;
}

/// Configuration for the transcription engine.
class TranscriptionConfig {
  /// Whisper model size: "tiny", "base", "small", "medium", "large-v3".
  final String modelSize;

  /// Language hint (empty for auto-detect).
  final String language;

  /// Whether to translate to English.
  final bool translateToEnglish;

  /// Minimum confidence threshold to emit segments.
  final double minConfidence;

  /// Audio chunk duration in milliseconds for processing.
  final PlatformInt64 chunkDurationMs;

  /// Use GPU acceleration if available.
  final bool useGpu;

  const TranscriptionConfig({
    required this.modelSize,
    required this.language,
    required this.translateToEnglish,
    required this.minConfidence,
    required this.chunkDurationMs,
    required this.useGpu,
  });

  static Future<TranscriptionConfig> default_() =>
      RustLib.instance.api.crateApiTranscriptionTranscriptionConfigDefault();

  @override
  int get hashCode =>
      modelSize.hashCode ^
      language.hashCode ^
      translateToEnglish.hashCode ^
      minConfidence.hashCode ^
      chunkDurationMs.hashCode ^
      useGpu.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is TranscriptionConfig &&
          runtimeType == other.runtimeType &&
          modelSize == other.modelSize &&
          language == other.language &&
          translateToEnglish == other.translateToEnglish &&
          minConfidence == other.minConfidence &&
          chunkDurationMs == other.chunkDurationMs &&
          useGpu == other.useGpu;
}
